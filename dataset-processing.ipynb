{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing dataset -- Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import json\n",
    "import pandas\n",
    "from chunkipy import TextChunker, TokenEstimator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/train.doj_guidance.jsonl.xz\"\n",
    "\n",
    "docs = []\n",
    "with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        docs.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "print(\"First document preview:\", docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys (fields) of the first document\n",
    "print(docs[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_save(text, chunk_size, tokens, overlap_percent, docId, output_dir=\"resources/data/chunks/guidance\"):\n",
    "    \"\"\"\n",
    "    Splits the input text into overlapping chunks and saves each chunk as a JSON file.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be chunked.\n",
    "        chunk_size (int): The size of each chunk.\n",
    "        tokens (bool): Whether to chunk by tokens.\n",
    "        overlap_percent (float): Percentage of overlap between chunks.\n",
    "        document_name (str): Name of the original document.\n",
    "        output_dir (str): Directory to save the chunk files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    text_chunker = TextChunker(chunk_size, tokens=tokens, overlap_percent=overlap_percent)\n",
    "    chunks = text_chunker.chunk(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"document_category\": \"guidance\", #change here\n",
    "            \"docId\": docId,\n",
    "            \"chunk_index\": i + 1,\n",
    "            \"chunk_text\": chunk\n",
    "        }\n",
    "        chunk_filename = f\"{docId}_chunk_{i + 1}.json\"\n",
    "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "        with open(chunk_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunk_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {chunk_filename} ({len(chunk)} chunk characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over docs\n",
    "for idx, doc in enumerate(docs):\n",
    "    url = doc['url']\n",
    "    docId = \"guidance_\"+str(idx)\n",
    "    chunk_and_save(doc['text'], chunk_size=500, tokens=True, overlap_percent=0.1, docId=docId, output_dir= \"resources/data/chunks/guidance\")\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chunkipy \n",
    "%pip install python-terrier\n",
    "%pip install pyterrier_pisa\n",
    "%pip install git+https://github.com/terrierteam/pyterrier_dr.git\n",
    "%pip install git+https://github.com/terrierteam/pyterrier_t5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyterrier_doc2query import Doc2Query, QueryScorer, QueryFilter\n",
    "from pyterrier_dr import ElectraScorer\n",
    "import json\n",
    "import os\n",
    "\n",
    "def append_queries(input_data):\n",
    "    for row in input_data:\n",
    "        row[\"querygen\"] = row[\"querygen\"].split(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def list_all_files(directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "def batch_processing(batch, pipeline):\n",
    "\n",
    "    # generate queries\n",
    "    queries = pipeline(batch)\n",
    "\n",
    "    # modify data structure\n",
    "    for query in queries:\n",
    "        if \"querygen_score\" in query:\n",
    "            query[\"querygen_score\"] = [str(num) for num in query[\"querygen_score\"]]\n",
    "    append_queries(queries)       \n",
    "\n",
    "    return queries\n",
    "\n",
    "def process(input_data_path, output_data_path, pipeline, batch_size):\n",
    "\n",
    "    # list all docs\n",
    "    docs = list_all_files(input_data_path)\n",
    "    \n",
    "    n_batches = len(docs) // batch_size + 1 * ( len(docs) % batch_size > 0 )\n",
    "\n",
    "    print(f\"Total number of docs: {len(docs)}\")\n",
    "\n",
    "    destination_files = list_all_files(output_data_path)\n",
    "\n",
    "    batch_num = 1\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "\n",
    "        # check if batch already processed\n",
    "        if f\"{output_data_path}/batch_{batch_num}.json\" in destination_files:\n",
    "            continue\n",
    "\n",
    "\n",
    "        files_batch = []\n",
    "\n",
    "        for file in docs[i:i+batch_size]:\n",
    "            # Open and read the JSON file\n",
    "            with open(file, 'r') as file_n:\n",
    "                files_batch.append(json.load(file_n))\n",
    "\n",
    "        docs_batch = []\n",
    "\n",
    "        for file in files_batch:\n",
    "            doc = {\n",
    "                \"id\":f\"{file['docId']}__{file['chunk_index']}\",\n",
    "                \"doc_id\": file[\"docId\"],\n",
    "                \"chunk_id\": file[\"chunk_index\"],\n",
    "                \"document_category\": file[\"document_category\"],\n",
    "                \"text\": file[\"chunk_text\"]\n",
    "            }\n",
    "            docs_batch.append(doc)\n",
    "        \n",
    "        # process batch\n",
    "        batch_queries = batch_processing(docs_batch, pipeline)\n",
    "\n",
    "        # save batch\n",
    "        with open(f\"{output_data_path}/batch_{batch_num}.json\", \"w\") as outfile:\n",
    "            json.dump(batch_queries, outfile, indent=2)\n",
    "        \n",
    "        print(f\"Batch processed : {batch_num} / {n_batches}\")\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_PATH = \"./chunks\"\n",
    "OUTPUT_DATA_PATH = \"./queries\" # TO BE CREATED IN ADVANCE (or programmaticaly)\n",
    "BATCH_SIZE = 32 # DO NOT CHANGE AFTER FIRST RUN\n",
    "N_QUERIES = 3\n",
    "\n",
    "# pipeline\n",
    "doc2query = Doc2Query(num_samples=N_QUERIES)\n",
    "scorer = ElectraScorer()\n",
    "\n",
    "# inspection\n",
    "pipeline = doc2query >> QueryScorer(scorer) # >> QueryFilter(append=False, t=3.21484375) # 30% electra filter\n",
    "\n",
    "process(INPUT_DATA_PATH, OUTPUT_DATA_PATH, pipeline, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "base_Folder = \"resources/data/querygen\"\n",
    "filenames = next(os.walk(base_Folder), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "\n",
    "chunks = []\n",
    "queries = []\n",
    "labels = []\n",
    "query_count = 0\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(base_Folder, filename), \"r\") as file:\n",
    "        chunks_batch = json.load(file)\n",
    "        for chunk in chunks_batch:\n",
    "            labels_chunk = []\n",
    "            chunk_queries = [q for q, s in zip(chunk[\"querygen\"], chunk[\"querygen_score\"]) if float(s) >= 1.5]\n",
    "            for chunk_query in chunk_queries:\n",
    "                queries.append({\"query_id\": query_count, \"query\": chunk_query})\n",
    "                labels_chunk.append({\"query_id\": query_count, \n",
    "                           \"doc_id\": chunk[\"doc_id\"],  \n",
    "                           \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                           \"label\": \"Relevant\"})\n",
    "                query_count += 1\n",
    "            del chunk[\"querygen\"]\n",
    "            del chunk[\"querygen_score\"]\n",
    "            chunks.append(chunk)\n",
    "            labels.extend(labels_chunk)\n",
    "            \n",
    "            \n",
    "documents_df = pd.DataFrame(chunks)\n",
    "documents_df.rename(columns={\"document_category\": \"category\", \"text\": \"content\"}, inplace=True)\n",
    "documents_df.to_csv(\"dataset/legal-docs/document.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "queries_df = pd.DataFrame(queries)\n",
    "queries_df.set_index('query_id', inplace=True)\n",
    "queries_df.to_csv(\"dataset/legal-docs/query.csv\", index=True, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "labels_df = pd.DataFrame(labels)\n",
    "labels_df.to_csv(\"dataset/legal-docs/positive-label.csv\", index=False, encoding=\"utf-8\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_df = pd.read_csv(\"dataset/legal-docs/document.csv\")\n",
    "queries_df = pd.read_csv(\"dataset/legal-docs/query.csv\")\n",
    "positive_labels_df = pd.read_csv(\"dataset/legal-docs/positive-label.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels_df[[\"doc_id\",\"chunk_id\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create negative labels\n",
    "\n",
    "import random\n",
    "from pyterrier_doc2query import QueryScorer\n",
    "from pyterrier_dr import ElectraScorer\n",
    "\n",
    "def create_negative_labels(labels_df, documents_df, quires_df, query_scorer):\n",
    "\n",
    "    query_id = labels_df[\"query_id\"].tolist()\n",
    "    random.shuffle(query_id)\n",
    "\n",
    "    # build df for Query Scorer\n",
    "    query_list = [{\n",
    "        \"query_id\": query_id[i],\n",
    "        \"doc_id\": labels_df['doc_id'][i],\n",
    "        \"chunk_id\": labels_df['chunk_id'][i],\n",
    "        \"text\": documents_df[documents_df[\"id\"] == f\"{labels_df['doc_id'][i]}__{labels_df['chunk_id'][i]}\"][\"content\"].values[0],\n",
    "        \"querygen\": quires_df[quires_df[\"query_id\"] == query_id[i]][\"query\"].values[0]\n",
    "    }\n",
    "    for i in range(len(labels_df))\n",
    "    ]\n",
    "\n",
    "    scores_df = query_scorer.transform(pd.DataFrame.from_records(query_list))\n",
    "\n",
    "    return scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 1\n",
    "\n",
    "negative_labels_df = pd.DataFrame()\n",
    "\n",
    "for _ in range(n_runs):\n",
    "     negative_labels_df = pd.concat((negative_labels_df,\n",
    "                                     create_negative_labels(positive_labels_df, documents_df, queries_df, QueryScorer(ElectraScorer()))\n",
    "                                     )\n",
    "                                    )\n",
    "negative_labels_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create relevance\n",
    "relevance = []\n",
    "for i in range(len(negative_labels_df)):\n",
    "    if negative_labels_df['querygen_score'][i][0] > 1.5:\n",
    "        relevance.append(\"Relevant\")\n",
    "    else:\n",
    "        relevance.append(\"Irrelevant\")\n",
    "\n",
    "\n",
    "negative_labels_df[\"label\"] = relevance\n",
    "negative_labels_df = negative_labels_df[positive_labels_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat with positive labels\n",
    "labels_df = pd.concat((positive_labels_df, negative_labels_df))\n",
    "labels_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.to_csv(\"dataset/legal-docs/label.csv\", index=False, encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
